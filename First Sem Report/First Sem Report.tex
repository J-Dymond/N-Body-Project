\documentclass[a4paper,10pt]{article}

\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{cleveref}
\usepackage{tabularx}
\usepackage{siunitx}
\usepackage{mathabx}
\usepackage{float}
\usepackage{subfig}
\usepackage[]{algorithm2e}

\usepackage{multicol}
\usepackage{etoolbox}
\patchcmd{\thebibliography}{\section*{\refname}}
    {\begin{multicols}{2}[\section*{\refname}]}{}{}
\patchcmd{\endthebibliography}{\endlist}{\endlist\end{multicols}}{}{}

\setlength{\oddsidemargin}{0cm}
\setlength{\evensidemargin}{0cm}
\setlength{\voffset}{-2.5cm}
\setlength{\textwidth}{16cm}
\setlength{\textheight}{25cm}
\setlength{\parskip}{0.5em}
\setlength{\parindent}{1.5em}

\usepackage[font=footnotesize,labelfont=bf]{caption}

\title{N-Body Simulations of the Solar System -- First Semester Report}

\author{Jack Dymond - 150194462}

\date{\today}

\begin{document}
\maketitle

\begin{abstract}
A literature search was performed, and progress made on creating a 4th order predictor corrector algorithm. The first steps were creating an efficient, and accurate second order N-body code, to help initialise the non-self-starting predictor corrector. A plan was also created, to help organise my time in the second semester. 
\end{abstract}

\section{Literature Review}

In order to undertake the project it was necessary to first inform myself of the N-Body problem, and its pertinence in Astrophysical problems. To do this, a literature review was undertaken.

\subsection{The N-Body Problem}

Consider a system of particles, or Bodies, in a dynamic system. In order to understand how the system will behave, the forces acting on each particle need to be understood, as well as how these forces vary with time. In very simple systems, namely those involving only two particles, there is often an analytical solution to understand how it will behave. Such as the Bohr model of an atom. However, when multiple particles are introduced, more than three to be specific, the problem is impossible to solve analytically, without making simplifying assumptions.\footnote{Such a Rydberg atom in atomic Physics that makes the assumption of a `Hydrogenic' atom, simplifying the problem to the Bohr model.} Hence, the N-Body Problem.  

To overcome this, the equations of motion governing the system are instead solved numerically. When the problem was first identified In the early 17th century, solving these equations to obtain long term dynamical behaviour was unfeasible. This is because the number of interactions that need to be computed will scale quadratically with N. Consequently, it wasn't until the advent of computers that N-body simulations began to proliferate in the Astrophysics community.

\subsection{The N-Body Problem in Astrophysics}

In Astrophysics the distance scales being considered are often so great, that gravity is the only force that contributes to a system's behaviour. Hence, astrophysical problems that analyse the dynamics of gravity dominated systems, are the ideal candidate for N-Body simulations.

The latter stages of planet formation is perhaps the smallest scale at which N-Body simulations become useful. In the former stages, particle sizes are very small, leaving millions of particles to account for. Hence, fluid mechanics are a more computationally feasible way of modelling them. However, this changes once in the runaway and oligarchic growth stages: There is an onset of more significant gravitational forces, and decrease in number of bodies. This sees N-Body simulations become a much more effective method. 

E. Kokubo and S. Ida were among the first to show this, in their paper \textit{On Runaway Growth of Planetesimals}\cite{Runaway}. They performed 3-Dimensional N-body simulations of 3000 equally massive planetesimals, orbiting a central body. The simulations provided conclusive evidence of naturally occurring runaway growth. This is something only previously inferred by statistical analysis of the coagulation condition\cite{Statistics,Coagulation}. The results thereby confirmed the validity of a statistical approach to demonstrate runaway growth. It also highlighted the disadvantages of the statistical approach in later stages of runaway growth, where the planetesimals are clustered together: A statistical approach assumes a homogenous distribution of planetesimals.

Once planetary systems are formed they are far from stable, understanding these instabilities, and how they evolve dynamically are another use for N-body simulations. 

One example of such instabilities is our early solar system, and studying its dynamics is an active area of research. Namely, unravelling the mysteries of its structure, such as the existence of the Kuiper Belt and the Oort cloud. But also explaining its history, in events such as the Late heavy Bombardment. The Nice model solves all of these, and is an example of how N-body simulations can provide answers to problems requiring more specificity to solve. 

The nice model, first hypothesised in 2005\cite{Nice}, describes an event in which the outer gas giants, Uranus and Neptune, undergo considerable migrations. The inner gas giants get into a 1:2 resonance, causing dynamical instabilities in the outer orbits. Consequently, Neptune is propelled past Uranus, as both planets moved to their current positions. These migrations disband an early Kuiper belt, pushing asteroids and planetesimals into the inner system, as well as out to its far reaches. Thereby explaining both the Oort Cloud, and the Late Heavy Bombardment. 

The principal investigative method used in the 2005 paper was the N-Body simulations of an early solar System. K. Tsiganis et al. performed N-Body simulations of the system, including with it a circumstellar disk of planetesimals, which was truncated at 30 AU. They found orbital resonances in Jupiter and Saturn would cause excitations in their own orbits, and instabilities in those of Uranus and Neptune. Once relaxed, in 50\% of the simulations the system would diverge to its current architecture. Perhaps most importantly, the simulations also reproduced the current eccentricities found in the orbits of the Gas Giants, something previously unexplained by planet formation theories.

\begin{figure}[h!]
\centering
\includegraphics[width=15cm]{"/Users/JackDymond/Documents/Work/Physics Modules/Fourth Year/PHY480/Figures/Nice"}
\caption{An illustration of the Nice model, it shows orbital distance on each exis (Given in AU). (a) through (c) shows the time evolution of the solar system. It is clear that Uranus (Light Blue) and Neptune (Dark Blue) switch positions in the simulation.}
\label{NiceFig}
\end{figure}

This was a high impact paper; since being published it's been cited 717 times. One of which was investigating the initial conditions of the model. 

The early solar system is inherently unobservable, meaning its initial conditions will always remain a mystery. However, a paper by Batygin et al. aimed to put constraints on them\cite{Initial}. They tried to find \textit{fully resonant} initial conditions, that post-evolution, would result in the architecture of our current system. Identifying which of these systems evolved in ways that matched the Nice model, was also one of their aims. In their findings they found 4 fully resonant initial conditions that were compatible with the Nice model, and 4 that inferred different evolutions. Their paper explained the importance of multi-resonant systems in the migration of the planets, and made predictions of the eccentricity jumps and evolutionary paths that given resonances would infer. These hypotheses were then supported using N-Body simulations of such systems.

With the Nice model being so well supported by subsequent work, it is now important for future developments to agree with it. 

One such development is the advent of `Planet 9'. Planet 9 refers to a planet several earth sizes in mass, orbiting in the far reaches of the solar system. There are many theories that allow for a 9th planet, such as an ejection early in the solar system's dynamical evolution. A much simpler solution however, would be the capture of a 9th planet from the Sun's birth cluster.  

The capture of a 9th planet would agree with current theories of the solar system, so long as certain criteria are met. These criteria are detailed at the beginning of a paper by Mustill et al.\cite{Planet 9}: The star--star encounter must be at a distance that would not disrupt the Kuiper belt ($\sim$150 AU); The other star must have a planet on a very wide orbit ($\geq$ 100 AU); Lastly, the planet must be captured onto a trajectory that allows for the current orbital distributions of the gas giants. The paper shows, through N-body simulations of sun--star encounters, that all of these criteria could be satisfied in a capture event, albeit at a small probability. 

In order for a planet to be captured from a foreign star, the sun would likely need to be in a star cluster at the time of capture. These are once again gravitationally dominated systems. Hence, studying their dynamics is encapsulated by N-body simulations.

The dynamics between stars in clusters, and that of the planets encompassing them, are often intertwined physically. This is true to such an extent, that the structure of planetary systems can infer the dynamics of the clusters in which they formed. 

Perturbations will often effect the orbits of young planetary systems, and these perturbations will persist long after the parent cluster has evaporated. The outer orbits of these systems will relax very slowly, and can hence have the potential to carry signatures of their cluster environment. In a paper published by Cai and his colleagues, this hypotheses is supported in detail\cite{Signatures}.

N-body simulations were performed of star clusters of varying stellar mass, between 0.04 M\textsubscript{\(\odot\)} and 25 M\textsubscript{\(\odot\)}. These stars hosted multi-planet systems using two different models. The distribution of semi-major axes remained consistent between these models, starting at 5.2 AU, and extending outwards to $\sim$200 AU. The planetary masses were different in each model, one used Jupiter mass planets, the other $\frac{1}{3}$ Earth mass planets. Simulations were used to observe a departure from these initial conditions, these departures arising due to both perturbations from stars, and through planet-planet scattering.

They found that the birth environment of a planetary system can be understood from observed orbital elements. They also suggested that the star clusters cause `natural selection' between the planetary systems formed within them, where only the most resilient systems would survive. Furthermore, the more dense stellar environments, such as deep inside the cluster, resulted in stronger signatures in the planetary orbits. With these statements they also provided quantitative relationships between the perturbation method, and the inclination and eccentricity of planetary orbits: They found that the inclination and eccentricity would rise systematically when subjected to external perturbations; Or if internal evolution dominated, the inclination would fluctuate in anti-phase with the eccentricity, this mechanism a result of the conservation of angular momentum. All of these findings were true regardless of which mass planet was used. These results also allowed the group to hypothesise the Sun's birth environment: The outskirts of a star cluster $\sim$1000 stars in size.

It follows that the reverse is also true, perhaps by observing stars in clusters, you could infer properties of planetary systems encompassing them, using the star's location in the cluster. 

Photometric properties of clusters can be measured from observations, these properties are often used to predict physical parameters, such as mass and radii. However, these are often estimations, and to obtain values with certainty, many parameters need to be measured to a high degree of accuracy.

The N-body method sees another use in supporting values estimated using observations, this is shown by H. Baumgardt, in a paper published in 2016\cite{Baumgardt}. Baumgardt performs 900 N-body simulations of different star clusters, with their concentration, size, and black hole mass fraction varying between simulations. 

By comparing data obtained from observations, against that provided through N-body simulations, the best fitting model was obtained for each cluster. With these models absolute masses could be derived, and hence their mass-light ratios. The presence of black holes in their simulations allowed predictions to be made, regarding the presence of black holes in observed clusters. Furthermore, cluster distances could be ascertained for galaxies with more accurate velocity dispersion measurements.

They found theoretically predicted mass-light ratios were in strong agreement with those predicted using N-body methods. For the majority of clusters, comparisons with N-body simulations didn't support the existence of black holes. One cluster however, $\omega$ Cen, strongly disagreed with N-body models that lacked a large black hole mass fraction, suggesting the cluster hosts a black hole at its centre, the paper suggesting a mass of $\sim$40,000 M\textsubscript{\(\odot\)}.

While being very useful to support findings from observations, the N-body method is most powerful when being used to model the evolution of these environments. 

An astrophysical body often investigated is the binary star system, and they prove to be of interest in many environments. Clusters by definition have a dense star population, and hence carry the potential to host many binary systems. The binary systems seen in clusters are often in the form of \textit{intermediate} binaries, these are binaries with separations of 10s--100s AU. 

The initial mass function in star formation is universal, but it is unclear as to whether this carries to stars found in binaries, as they are frequently separated in clusters due to their often high separations. Parker and Goodwin investigated this\cite{Binaries}.

N-body simulations were performed of clusters with 100\% binary population, each with separations in the range of 62--620 AU. The primary and secondary mass functions used, were derived from previous work in the field. They allowed these clusters to evolve dynamically over a period of $\num{e6}$ years, and then reassessed the binary populations. 

Two cluster morphologies were used, the more sub-structured fractal clusters, and the centrally dominated Plummer spheres. In each cluster environment, ten simulations would be performed for a given initial binary population, whilst varying the positions and velocities of the individual binary systems. 

After the simulations they found the binary population could decrease by a factor of 2 after 1 Myr, separation distances could also change to``statistically significant" values over this time: In the field these separation distances are sometimes used to infer dynamical history and initial conditions. This paper shows caution should be taken when doing this, as even with the full knowledge of initial conditions, the dynamical evolution of clusters often created statistical outliers. That is to say, when observing binary separation distances, you cannot be sure if this is due to initial conditions, or due to a particularly unusual dynamical evolution.

In particularly active environments, not only are stars seen in binaries, but also the clusters themselves. In fact $\sim$10\% of clusters are found in binary pairs, these are known as \textit{binary clusters}. 

In a paper by B. Arnold et al. the formation of these systems is investigated\cite{ClusterBinary}. The group simulated systems of 1000 stars, to emulate star forming regions. This was done over a period 20 Myr, the structure of the cluster was analysed during, and after its dynamical evolution. They varied the initial degree of sub-structuring, as to capture both smooth, and more hierarchical distributions. Their viral ratios\footnote{This is the ratio of Kinetic to potential energy} were also varied, from relatively `cool' values of 0.3, to `hot' values of 0.7. The velocity distribution was such that stars were given velocities similar to that of stars close to them spatially, known as `coherence'. As a method of analysing the resultant morphology, three different classifications were given: \textit{Binary Clusters}, and \textit{Single clusters}, these being self evident. The final class was less so: \textit{Binary Merger}. In this paper, a binary merger specifies a cluster that was once a binary at some point throughout its evolution, but merged over the 20 Myr period.

They found that binary clusters could form from these star forming regions, and that this would be the case more frequently if the cluster was initially `hot' dynamically. However, they also noted this was a very stochastic mechanism: It was impossible to determine whether a binary would form, and indeed which stars occupy the two clusters, from the initial conditions alone. When binaries did form, they found that they'd often drift apart from one another, and that initial substructure would significantly influence the mass ratio of the binaries. They found high amounts of substructure gave high/low mass ratios, no substructure gave more equal mass ratios. The most equal mass ratios came from regions with a small amount of initial substructure. In the event that clusters remained bound to one another, they would usually recombine later in time, this was mostly the case in systems that began in virial equilibrium.

Clusters are very interesting astrophysical objects, that can provide information about astrophysical problems, from the formation of planets, to the formation and evolution of galaxies. However these areas are often complicated by the dominance of turbulence, something the N-body models haven't considered in the papers discussed.

Galaxies are often dominated by dark matter, something which traditional N-body models can't simulate, due to its diffuse, fluid like behaviour. Being so dominant in many large scale processes, dark matter cannot be ignored at the large scale. Hence, a model was developed to assist in simulations of galaxies, as well as those at the cosmological scale. 

GADGET (\textbf{GA}laxies with \textbf{D}ark matter and \textbf{G}as int\textbf{E}rac\textbf{T}), is a tool, first developed in 2001\cite{Gadget}, to allow N-body simulations to be performed while still considering dark matter and gas distributions. This was achieved by implementing smoothed particle hydrodynamics, a lagrangian technique which is effective in solving hydrodynamical problems. Which was used in conjunction with the simulation of individual particles. This could be used with collisionless particles, therefore capturing dark matter in the scope of the code's capabilities. 

The code has seen prolific use in the field, quite recently a more advanced version of the code, GADGET-3, was used in simulations of the Milky Way. 

The Milky way's disk shows vertical warps, this is highlighted at the beginning of a paper by M. Chequers et al.\cite{Bendy}. The paper presented a method of analysing wave like perturbations in the disk. They also hypothesised that these warps may be caused by inhomogeneities in the dark matter distribution in the halo. 

They perform simulations of smoothly distributed halos, as well as those with more substructure, in form of dark matter satellites, `sub-halos'. They found the bending of the disk was quantitatively similar in both halo distributions. Although the amplitudes were amplified when satellites were present in the halos. This is shown in \cref{Disk}, a figure lifted directly from the paper. It is clear the effect is more pronounced in the `satellites' simulation.

\begin{figure}[h!]
\centering
\includegraphics[width=15cm]{"/Users/JackDymond/Documents/Work/Physics Modules/Fourth Year/PHY480/Figures/Disk"}
\caption{The evolution of the disk is shown over a period of 10 Gyr. This is presented in two simulations, `Isolated' and `satellites', these labels indicating the presence of dark matter halos. The radial position is marked at 5 kpc increments with dotted concentric circles. The mean vertical displacement in each simulation is highlighted using a colour coded shading on the galaxy maps. With the colour legend shown to the right.}
\label{Disk}
\end{figure}

To better understand the linkage between the halo and the disk, they propose the simulations be used in conjunction with observations, as is so often the case in N-body research in astrophysics.

It is clear from this literature survey that the N-body method is a very powerful tool in astrophysics, being prevalent from the planetary scale, to the cosmological, its scope is unparalleled in the field. The algorithmic complexity necessary to efficiently run these simulations is also very high, especially when hydrodynamic systems are implemented in parallel. Hence using these methods in this project will prove invaluable in the pursuit of a career in the academic and industrial environment alike. \pagebreak[4]


\section{Progress on Project}

There are a many ways to approach the N-Body problem to the 4th order. More accurate methods are predictor-correctors, in particular the Hermite Polynomial, and the Adams-Bashforth-Moulton model. The former tries to predict the higher order kinematic components, the aptly named Jerk, Snap, Crackle, and Pop. This is computationally expensive, as crossproducts between velocity and acceleration vectors are needed. This is something relatively inefficient for computer processors to calculate. The Adams-Bashforth-Moulton method predicts future values by fitting a polynomial to previous values. This is less expensive computationally, but requires storing previous values of the quantity that needs predicting, 8 values to be exact. 

At low values of N, storing these values is more computationally feasible hence, the method is more efficient than the Hermite Method. For this reason, this project is going to implement the Adams-Bashforth-Moulton model. This method is not self starting, meaning in order to function it requires time-history of the velocity and acceleration vectors. In order to do this, a second order method is necessary to initialise the values used for a time history. This a process known as `bootstrapping'. The first part of the project required implementing an accurate 2nd order N-body code, to be used for bootstrapping. 

\subsection{Second order Kinematics}

In order to calculate a body's changes in velocity and position over a given time-step, the acceleration vector acting on it must first be calculated. In a system dominated by gravitational forces, this requires knowledge of every body's position in a consistent coordinate system. For a given body, $\alpha$, its interaction with another body $\beta$, can be calculated using Newton's Law of gravitation, provided the distance between them, $\vec{r_{\alpha\beta}}$, is known. By summing over all interactions the body will have, its total acceleration vector can be calculated as shown in \cref{Sum_a}.

\begin{equation} \label{Sum_a}
    \vec{a_\alpha} =  \sum\limits_{\beta\neq \alpha}^{N} \frac{G m_\beta}{\left|\vec{r_{\alpha\beta}}\right|^3}\vec{r_{\alpha\beta}}
\end{equation}

Here, $N$ denotes the total number of particles, $m$ the mass, and $G$ the Gravitational constant. A factor of $\vec{r_{\alpha\beta}}$ is applied, along with the cube of the absolute distance in the denominator. This acts as a unit vector in the direction of the acceleration.\newline  


Given a discrete time-step over which to apply this acceleration, the changes in distance and velocity can be determined using the kinematic equations. These are shown below, where the subscripts $i$ and $f$ denote the initial and final positions respectively, $\delta t$ represents the time-step, and $\vec r$, $\vec v$ and $\vec a$, the position, velocity and acceleration. %\cref{PositionFD} and \cref{VelocityFD}

\noindent\begin{tabularx}{\textwidth}{@{}XX@{}}
  \begin{equation*}
   \vec{r}_{\alpha,f} =  \vec{r}_{\alpha,i} + \vec{v}_{\alpha} \delta t
    \label{PositionFD}
  \end{equation*} &
  \begin{equation*}
 \vec{v}_{\alpha,f} =  \vec{v}_{\alpha,i} + \vec{a}_{\alpha} \delta t
    \label{VelocityFD}
  \end{equation*}
\end{tabularx}

For the position, this can easily be extended to the second order, by including the current acceleration. This is shown in \cref{PositionSD}.

\begin{equation} \label{PositionSD}
    \vec{r}_{\alpha,f} =  \vec{r}_{\alpha,i} + \vec{v}_{\alpha} \delta t + \frac{1}{2}\vec{a}_{\alpha}\delta t^2
\end{equation}

To calculate the velocity to the second order, the rate of change of the acceleration is required, the jerk. This is not a simple calculation, and is computationally expensive. Hence, an approximation should be made instead: $\frac{da}{dt} \approx \frac{a_i - a_f}{\delta t}$. Given a small change in acceleration this in a valid assumption. Programatically, this requires storing the previous value of acceleration, $a_p$. By using it in conjunction with the current acceleration, $a_c$, the second order equation for the velocity can now be taken as \cref{VelocitySD}.

\begin{equation} \label{VelocitySD}
     \vec{v}_{\alpha,f} \;\; =  \;\; \vec{v}_{\alpha,i} + \vec{a}_{\alpha} \delta t + \frac{1}{2}\frac{d\vec{a_\alpha}}{dt}\delta t^2 \;\; = \;\; \vec{v}_{\alpha,i} + \vec{a}_{\alpha,c} \delta t + \frac{1}{2}(\vec{a}_{\alpha,p} - \vec{a}_{\alpha,c})\delta t
\end{equation}

\subsection{Recreating the Solar System accurately}

To undertake N-Body simulations, these are the only equations necessary. However, to start the process, initial conditions should be given to each body, namely a mass, position, and velocity. These were taken from a NASA database\cite{PlanetFacts}, see \cref{cond} for the values used in this project.  

To initialise the simulation, the planets were positioned on the $x$ axis at their orbital radii, and at $y=0$. Their respective orbital velocities were then applied along the $y$ axis, with $v_x = 0$. Hence, when the simulation was run they would orbit on the $x$--$y$ plane. 

At the beginning of each time-step, the acceleration acting on each body was calculated using \cref{Sum_a}, this value was then applied to the kinematic equations, \cref{PositionSD}, and \cref{VelocitySD}. The changes in position and velocity were then applied to each body, new accelerations were calculated, and the process was repeated. Since no time history was given, initially the change in acceleration couldn't be calculated in \cref{VelocitySD}. For this reason, only the first derivative of velocity was used in the calculation of the second value of velocity, $\vec v_{\alpha,2}$. This was done by manually setting the change in acceleration to zero, for the first timestep.

To demonstrate the accuracy of this second order code, a 1000 year simulation of the solar system was carried out with 10 second time-steps. The resulting simulation produced a system with stable orbits, this is reflected in \cref{Distance}. \Cref{Distance} (a) shows the log of orbital distance against time, it is clear the orbits remain at a consistent distance with only Earth drifting very slightly. The oscillations in the distances of Saturn, Uranus, and Neptune, are due to the elliptical nature of their orbits, the eccentricities were reasonable: $\sim$0.02, $\sim$0.04, and $\sim$0.03 respectively. \Cref{Distance} (b) shows the orbits in the $x$--$y$ plane, demonstrating their circular nature. 

\begin{figure}[h!]
    \centering
    \subfloat[Orbital distances against time. Elapsed time is given on the $x$ axis, and the log of the distance is presented on the $y$ axis. These are given in Years and AU respectively.]{{\includegraphics[width=8cm]{"/Users/JackDymond/Documents/Work/Physics Modules/Fourth Year/PHY480/Figures/SecondOrderDis"} }}%
    \qquad
    \subfloat[Face-on map of the system. This shows the $x$-$y$ position of the orbits, given in AU.]{{\includegraphics[width=7cm]{"/Users/JackDymond/Documents/Work/Physics Modules/Fourth Year/PHY480/Figures/SecondOrderOrbits"} }}%
    \caption{Plots of orbital position throughout the simulation.}%
    \label{Distance}%
\end{figure}

It should be noted, Venus and Mercury are rejected at this stage, as simulating them accurately is not computationally feasible. A much smaller time-step would be necessary to accurately recreate Venus' orbit, even smaller for that of Mercury. However, later in the project they will be put into the model, once the desired level of accuracy is achieved. \pagebreak

To further ensure the model's accuracy, energy conservation was considered. The total energy was calculated when the system was initialised to obtain $E_i$, this was simply the sum of the kinetic and potential energies, as shown in \cref{E_tot}. A factor of a half was applied to the potential energy, as in this summation it would be calculated twice for a given interaction $\alpha \leftrightarrow \beta$. 

\begin{equation} \label{E_tot}
    E_{tot}  \;\; =  \;\;  \sum\limits_{\alpha=1}^{N} \bigg[ \frac{1}{2}m_{\alpha}\left|v_{\alpha}\right|^2 - \sum\limits_{\beta\neq \alpha}^{N}\frac{G m_\beta m_\alpha}{2 \left|r_{\alpha\beta}\right|}\bigg]  \;\; = \;\; \frac{1}{2} \sum\limits_{\alpha=1}^{N}m_{\alpha} \bigg[\left|v_{\alpha}\right|^2 - \sum\limits_{\beta \neq \alpha}^{N}\frac{G m_\beta}{\left|r_{\alpha \beta}\right|}\bigg] 
 \end{equation}
 
 This value was recalculated throughout the simulation, and compared against the initial value, using \cref{compare}. Where $ \Delta E$, represents the fractional energy loss, and the subscript $t$ denoting the time-step at which the value is calculated.
 
 \begin{equation} \label{compare}
    \Delta E_t =  \frac{E_i - E_t}{E_i}
\end{equation}

This fractional energy loss was plotted against time, to see how it varied throughout the simulation. This is shown in \cref{Energy}. \Cref{Energy} (a) shows the variation across the entire simulation, whereas \cref{Energy} (b) shows the value over a 100 year time period. 

Over the whole simulation, the average energy loss remains close to zero, only lowering slightly, mainly due to the drift of Earth. However, there are clearly oscillatory effects in the value, these oscillations are due to interactions of Jupiter with Saturn. Upon increasing the time-resolution of this graph, \cref{Energy} (b), smaller scale oscillations can be seen. These are presumably a result of interactions between the other planets in the system. Nonetheless, the energy losses (and its fluctuation) proved to be minor in this simulation; the large scale oscillation having an amplitude of $\sim 0.0003$, and the average value only lowering by $\sim 0.0002$.

\begin{figure}[h!]
    \centering
    \subfloat[Energy loss over entire simulation. With time on the $x$ axis, and $\delta E$ on the y axis.]{{\includegraphics[width=7cm]{"/Users/JackDymond/Documents/Work/Physics Modules/Fourth Year/PHY480/Figures/SecondOrderEnergyLoss"} }}%
    \qquad
    \subfloat[Energy loss over 100 year period. Again with time on the $x$ axis, and $\delta E$ on the y axis.]{{\includegraphics[width=7cm]{"/Users/JackDymond/Documents/Work/Physics Modules/Fourth Year/PHY480/Figures/SecondOrderEnergyLossTime"} }}%
    \caption{Plots showing the Energy loss throughout the simulation.}%
    \label{Energy}%
\end{figure}

\subsection{Suitability for Bootstrapping}\label{Suitable}

The previous section demonstrates the code's ability to accurately recreate the orbits of the Solar System, to a reasonable degree of accuracy. However, if a simulation is to run for millions of years, the discrepancies seen in the 2nd order system will manifest as large inaccuracies. Hence, the 4th order Adams-Bashforth-Moulton method will be preferred.

As a method of producing the initial time history, the 2nd order code is more than sufficient; over a small timescale, $<80$ seconds, the errors seen at long timescales will be very small. Accordingly, this code will be used initially in the bootstrapping phase.   \pagebreak

\subsection{Randomising Initial Positions Along Orbit}

If the simulation is run multiple times, it will be beneficial to randomise the initial positions of the planets along their orbits. Should the code be applied to astrophysical problems, this would prevent any deterministic results from arising, thereby producing more natural behaviour. 

There are many ways to approach this, some requiring more knowledge of the orbital paths than others. However, with knowledge of the mean orbital radius, $R$, and velocity, $v_T$, the planet can be placed at any position along a ring of radius $R$. Provided the velocity vector is perpendicular to the radial vector, and has a magnitude equal to the mean orbital velocity. 

Consider the positive quadrant of a planet's orbit (see \cref{Distance} (b)). On a given axis, the position will take a value between 0 and $R$, and the velocity will take a value between 0 and $v_T$. The component on the other axis can then be calculated using trigonometry. This is represented mathematically as shown in \cref{Random}, where $X_{\alpha}$ represents the random number between 0 and 1, assigned to the body, $\alpha$.

\begin{align}\label{Random}
&r_{x,\alpha} = X_{\alpha} R_{\alpha}&
&\text{and}&
&v_{y,\alpha} = X_{\alpha} v_{T,\alpha}
\end{align}
Then by using Trigonometry, the components along the other axes can be calculated, as shown in \cref{Trig}. Note the opposite axis on $r_{\alpha}$, and $v_{\alpha}$ respectively in \cref{Random}, this is due to the velocity vector being perpendicular to the radius vector.
\begin{align}\label{Trig}
&r_{y,\alpha} = R_{\alpha}^2 - r_{x,\alpha}^2&
&\text{and}&
&v_{x,\alpha} = v_{T,\alpha}^2 - v_{y,\alpha}^2
\end{align}

To allow for the other three quadrants, a random factor of $\pm 1$ should be applied to each component of the position. The value applied to the velocity will depend on the value applied to the radius vector. That is to say, the quadrant allocated by the random factors of $\pm1$ will determine the value of $\pm1$ applied to the velocity components. 

This was achieved programatically by first randomly assigning a value of $\pm 1$ to each position vector component, a parity. Then, using a simple series of \textit{IF} statements, the velocity components were allocated corresponding parities. Thus creating two $1\times 2$ arrays for each quantity. The conditions for the algorithm are illustrated in \cref{Algorithm}.

\begin{table}[h!]
\centering
\def\arraystretch{1.7}
\begin{tabular}{c|c}
\multicolumn{1}{l|}{Position {[}x,y{]}} & \multicolumn{1}{l}{Velocity {[}$v_x$,$v_y$ {]}} \\ \hline
{[}1,1{]}                               & {[}-1,1{]}                                      \\ \hline
{[}-1,1{]}                              & {[}-1,-1{]}                                     \\ \hline
{[}-1,-1{]}                             & {[}1,-1{]}                                      \\ \hline
{[}1,-1{]}                              & {[}1,1{]}                                      
\end{tabular}
\caption{\label{Algorithm} Position parity vectors, with corresponding velocity parity vectors.}
\end{table}

Regardless of which quadrant they're in, the same trigonometric relations in \cref{Trig} exist between the quantities, and their components. Hence, with a random number allocated to each position component, the planet can be randomly positioned in any quadrant, and in any position along its orbit.

This functionality was added to the code, and the resultant behaviour was similar to that seen in \cref{Distance} and \cref{Energy}.

\subsection{Moving Forward}

As \cref{Suitable} states, this code will be more than sufficient for initialising the first 8 time-steps of a predictor-corrector. Furthermore, with planets positioned at random positions along their orbit, there is no possibility of a deterministic effects arising from the initial setup of the system. 

Hence, the project can continue towards the goal of creating a 4th Order Predictor Corrector. The logistics of this are detailed in the next section.
\pagebreak[4]

\section{Project Plan}

The Gantt chart is an effective method of visualising a project, its steps, milestones, and timescales over which they should be achieved. A Gantt chart for this project is presented below.

\begin{thebibliography}{9}

\bibitem{Runaway}
Kokubo, E., and S. Ida\\
On Runaway growth of Planetesimals: N-body simulation\\ 
Icarus 1996, 123, 180--191.

\bibitem{Statistics}
Barge, P., and R. Pellat\\ 
Mass spectrum and velocity dispersions during planetesimal accumulation. I. Accretion\\
Icarus 1991 93, 270--287

\bibitem{Coagulation}
Philip J. Armitage\\
Astrophysics of Planet Formation\\
Sec. 4.5.1, Pg. 131\\
10 Dec. 2009\\

\bibitem{Nice}
K. Tsiganis, R. Gomes, A. Morbidelli and H. F. Levison\\
Origin of the orbital architecture of the giant planets of the Solar System\\ 
Nature, Vol 435, P 469--451, 26 May 2005

\bibitem{Initial}
Konstantin Batygin and Michael E. Brown\\
Early Dynamical Evolution of the Solar System: Pinning Down the Initial Conditions of the Nice Model\\
 ApJ, 716:1323--1331, 2010 June 20\\

\bibitem{Planet 9}
Alexander J. Mustill, Sean N. Raymond, and Melvyn B. Davies\\
Is there an exoplanet in the Solar System?\\
MNRAS Issue 1, p.109--113\\

\bibitem{Signatures}
Maxwell Xu Cai, Simon Portegies Zwart, and Arjen van Elteren\\ 
The signatures of the parental cluster on field planetary systems\\
MNRAS, V. 474, Issue 4, 11 03 2018, P 5114--5121\\

\bibitem{Baumgardt}
H. Baumgardt\\
N-body modelling of globular clusters: masses, mass-to-light ratios and intermediate-mass black holes\\
MNRAS, V. 464, Issue 2, 11 January 2017, P 2174--2202.

\bibitem{Binaries}
Richard J. Parker. and Simon P. Goodwin\\
The same, but different: Stochasticity in binary destruction\\
MNRAS, V. 424, Issue 1, 21 July 2012, P. 272--281

\bibitem{ClusterBinary}
Becky Arnold, Simon P. Goodwin, D. W. Griffiths and Richard. J. Parker\\
How do binary clusters form?\\
MNRAS, V. 471, Issue 2, 21 October 2017, P. 2498--2507

\bibitem{Gadget}
Volker Springel, Naoki Yoshida, and Simon D. M. White\\
GADGET: a code for collisionless and gasdynamical cosmological simulations\\
New Astronomy, V. 6, Issue 2, April 2001, P. 79--117

\bibitem{Bendy}
Matthew H Chequers, Lawrence M Widrow, and Keir Darling\\
Bending waves in the Milky Way?s disc from halo substructure\\
MNRAS, V. 480, Issue 3, 1 November 2018, P. 4244--4258

\bibitem{PlanetFacts}
Williams, David\\
TPlanetary Fact Sheet - Metric\\
https://nssdc.gsfc.nasa.gov/planetary/factsheet/\\ As of: \today

\end{thebibliography}


\begin{appendix}

\section{Initial Conditions for Solar System}\label{cond}
\begin{table}[H]
\begin{tabular}{l|l|l|l}
Planet  & Mass (\num{e24} \si{\kilo\gram}) & Initial Orbital Velocity (\si{\kilo\metre\per\second}) & Initial Orbital Radius (AU) \\ \hline
Earth   & 5.97                                     & 29.8                                                & 1.00                        \\
Mars    & 0.642                                    & 24.1                                                & 1.41                        \\
Jupiter & 1898                                     & 13.1                                                & 5.03                        \\
Saturn  & 568                                      & 9.7                                                 & 9.20                        \\
Uranus  & 86.8                                     & 6.8                                                 & 18.64                       \\
Neptune & 102                                      & 5.4                                                 & 30.22                      
\end{tabular}
\caption{\label{InitConds}Initial Conditions taken from \cite{PlanetFacts}}
\end{table}

\section{Relationship to previous projects}
In my second semester 3rd year project, I used the N-body method to analyse the fragility of the solar system to external perturbations. This used code very similar to that in the first half of this project. Hence, I focussed on streamlining the process, making it more efficient, and increasing its functionality. I also focussed on creating a framework to implement it with a 4th order corrector, rather than collecting and collating results from simulations. 

Hence, while the projects overlap in the fundamental method used, my focus has been different in each project. With this 4th year project concentrating much more on the code itself, rather than implementing it to an astrophysical problem.

\end{appendix}


\end{document}
              
    
